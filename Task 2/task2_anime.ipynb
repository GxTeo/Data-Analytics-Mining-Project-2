{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "import sys\n",
    "sys.path.append('../Task 1/')\n",
    "from efficient_apriori import apriori\n",
    "from improved_apriori import Improved_Apriori\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "import itertools\n",
    "import ast\n",
    "from tqdm import tqdm\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the dataset by chunks into username and the list of animes the user watched\n",
    "def process_anime_chunk(df, carry_over):\n",
    "    if carry_over is not None:\n",
    "        df = pd.concat([carry_over, df])\n",
    "    groups = df.groupby('username')['title'].apply(list)\n",
    "    last_user = df.iloc[-1]['username']\n",
    "    if last_user in groups:\n",
    "        carry_over = df[df['username'] == last_user]\n",
    "        groups = groups.drop(last_user)\n",
    "    else:\n",
    "        carry_over = None\n",
    "    return groups, carry_over"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "carry_over = None\n",
    "chunksize = 100000 # adjust this value depending on your available memory\n",
    "\n",
    "if(not os.path.exists('dataset/processed_anime_output.txt')):\n",
    "    with open('dataset/processed_anime_output.txt', 'w') as f:\n",
    "        for chunk in pd.read_csv('dataset/final_animedataset.csv', chunksize=chunksize):\n",
    "            groups, carry_over = process_anime_chunk(chunk, carry_over)\n",
    "            for user, anime_list in groups.items():\n",
    "                f.write(f'{user} {anime_list}\\n')\n",
    "\n",
    "        # don't forget to process the last carry_over\n",
    "        if carry_over is not None:\n",
    "            groups, _ = process_anime_chunk(carry_over, None)\n",
    "            for user, anime_list in groups.items():\n",
    "                f.write(f'{user} {anime_list}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file_in_partitions(file_path, partition_size):\n",
    "    with open(file_path, 'r') as file:\n",
    "        partition = []\n",
    "        for line in file:\n",
    "            partition.append(line)\n",
    "            if len(partition) >= partition_size:\n",
    "                yield partition\n",
    "                partition = []\n",
    "        if partition:  # yield any remaining lines\n",
    "            yield partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variable to get the counts of all itemsets\n",
    "global_counts = {}\n",
    "def generate_global_counts(partition, global_candidates):\n",
    "\n",
    "    # For 1th itemset, generate the transaction id list for the ith partition \n",
    "    transaction_id_dict = collections.defaultdict(list)\n",
    "    for transaction_id in partition:\n",
    "        for item in partition[transaction_id]:\n",
    "            item_tuple = (item,)\n",
    "            transaction_id_dict[item_tuple].append(transaction_id)\n",
    "\n",
    "    # Filter based on the global candidates formed\n",
    "    transaction_ids_dict = {item: transaction_ids for item, transaction_ids in transaction_id_dict.items() if item in global_candidates[1]}\n",
    "\n",
    "    # Get the global count of all 1th itemset\n",
    "    for item in transaction_id_dict:\n",
    "        if(len(item) not in global_counts):\n",
    "            global_counts[len(item)] = {}\n",
    "        if(item not in global_counts[len(item)]):\n",
    "            global_counts[len(item)][item] = len(transaction_id_dict[item])\n",
    "        else:\n",
    "            global_counts[len(item)][item] += len(transaction_id_dict[item])\n",
    "\n",
    "    # Extend to find global count of all 2th itemset from the global candidates\n",
    "    for i in tqdm(range(1, len(global_candidates))):\n",
    "        for itemset in global_candidates[i+1]:\n",
    "            transaction_ids = set(transaction_id_dict[(itemset[0],)])\n",
    "            for i in range(1, len(itemset)):\n",
    "                # We are only interested in the transactions where all items in itemset is present\n",
    "                transaction_ids = transaction_ids.intersection(set(transaction_ids_dict.get((itemset[i],), {})))\n",
    "            if(len(itemset) not in global_counts):\n",
    "                global_counts[len(itemset)] = {}\n",
    "\n",
    "            if(itemset not in global_counts[len(itemset)]):\n",
    "                global_counts[len(itemset)][itemset] = len(transaction_ids)\n",
    "            else:\n",
    "                global_counts[len(itemset)][itemset] += len(transaction_ids)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition 1:\n",
      "Found 8745 candidate itemsets from 1st Level\n",
      "Found 54 frequent itemsets from 1th item candidate sets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1431/1431 [00:01<00:00, 874.61it/s]\n",
      "100%|██████████| 1151/1151 [00:02<00:00, 450.73it/s]\n",
      "100%|██████████| 289/289 [00:00<00:00, 309.99it/s]\n",
      "100%|██████████| 47/47 [00:00<00:00, 239.19it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 195.46it/s]\n",
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition 2:\n",
      "Found 8745 candidate itemsets from 1st Level\n",
      "Found 54 frequent itemsets from 1th item candidate sets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1431/1431 [00:02<00:00, 663.37it/s]\n",
      "100%|██████████| 1017/1017 [00:02<00:00, 450.01it/s]\n",
      "100%|██████████| 241/241 [00:00<00:00, 305.04it/s]\n",
      "100%|██████████| 36/36 [00:00<00:00, 234.14it/s]\n",
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition 3:\n",
      "Found 8745 candidate itemsets from 1st Level\n",
      "Found 38 frequent itemsets from 1th item candidate sets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 703/703 [00:00<00:00, 808.94it/s]\n",
      "100%|██████████| 415/415 [00:01<00:00, 391.88it/s]\n",
      "100%|██████████| 81/81 [00:00<00:00, 313.31it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 243.44it/s]\n",
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition 4:\n",
      "Found 8746 candidate itemsets from 1st Level\n",
      "Found 45 frequent itemsets from 1th item candidate sets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 990/990 [00:01<00:00, 840.40it/s]\n",
      "100%|██████████| 877/877 [00:02<00:00, 437.21it/s]\n",
      "100%|██████████| 289/289 [00:00<00:00, 309.24it/s]\n",
      "100%|██████████| 23/23 [00:00<00:00, 235.10it/s]\n",
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition 5:\n",
      "Found 8745 candidate itemsets from 1st Level\n",
      "Found 35 frequent itemsets from 1th item candidate sets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 595/595 [00:00<00:00, 903.80it/s]\n",
      "100%|██████████| 49/49 [00:00<00:00, 439.50it/s]\n",
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition 6:\n",
      "Found 8738 candidate itemsets from 1st Level\n",
      "Found 20 frequent itemsets from 1th item candidate sets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 190/190 [00:00<00:00, 919.76it/s]\n",
      "100%|██████████| 8/8 [00:00<00:00, 461.21it/s]\n",
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition 7:\n",
      "Found 8740 candidate itemsets from 1st Level\n",
      "Found 7 frequent itemsets from 1th item candidate sets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21/21 [00:00<00:00, 945.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition 8:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8681 candidate itemsets from 1st Level\n",
      "Found 6 frequent itemsets from 1th item candidate sets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:00<00:00, 856.71it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition 9:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8666 candidate itemsets from 1st Level\n",
      "Found 5 frequent itemsets from 1th item candidate sets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 944.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition 10:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8424 candidate itemsets from 1st Level\n",
      "Found 0 frequent itemsets from 1th item candidate sets\n",
      "Partition 11:\n",
      "Found 6 candidate itemsets from 1st Level\n",
      "Found 0 frequent itemsets from 1th item candidate sets\n"
     ]
    }
   ],
   "source": [
    "num_partitions = 10 \n",
    "file_path = 'dataset/processed_anime_output.txt'\n",
    "size_of_data = sum(1 for line in open(file_path))\n",
    "min_support=0.5\n",
    "partition_size = size_of_data // num_partitions\n",
    "partition_candidates = []\n",
    "global_candidates = collections.defaultdict(list)\n",
    "# Step 1: Partitioning\n",
    "for i, partition in enumerate(read_file_in_partitions(file_path, partition_size)):\n",
    "    print(f'Partition {i+1}:')\n",
    "    dict_anime = {}\n",
    "    for line in partition:\n",
    "        user, anime_list_str = line.strip().split(' ', 1)\n",
    "        anime_list = ast.literal_eval(anime_list_str)\n",
    "        dict_anime[user] = anime_list\n",
    "\n",
    "    improved_apriori = Improved_Apriori(dict_anime, min_support=min_support, min_confidence=1, verbose=0)\n",
    "    # # Step 2: Retreieve frequent itemset per partition\n",
    "    partition_frequent_itemset = improved_apriori.apriori()\n",
    "    # Form the global candidate set from the large itemset in each partitions\n",
    "    # In this space, we ignore the count of itemset in each partition as they are not useful in our global support count\n",
    "    # All they do is just show the itemset was large enough in the current partition\n",
    "    for level, itemset in partition_frequent_itemset.items():\n",
    "        for key in itemset.keys():\n",
    "            if(key not in global_candidates[level]):\n",
    "                global_candidates[level].append(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition 1:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:03<00:00,  1.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition 2:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:03<00:00,  1.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition 3:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:03<00:00,  1.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition 4:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:04<00:00,  1.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition 5:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:03<00:00,  1.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition 6:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:02<00:00,  2.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition 7:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:01<00:00,  3.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition 8:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:02<00:00,  1.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition 9:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:01<00:00,  3.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition 10:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 11.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition 11:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 2269.65it/s]\n"
     ]
    }
   ],
   "source": [
    "# Now we have to read the lines in chunks for our disk-based operations\n",
    "min_support_count = min_support * size_of_data\n",
    "for i, partition in enumerate(read_file_in_partitions(file_path, partition_size)):\n",
    "    print(f'Partition {i+1}:')\n",
    "\n",
    "    # Hold the partition data in main memory\n",
    "    # Pure disk based implementation would probably require us to save the partition in disk \n",
    "    dict_anime = {}\n",
    "    for line in partition:\n",
    "        user, anime_list_str = line.strip().split(' ', 1)\n",
    "        anime_list = ast.literal_eval(anime_list_str)\n",
    "        dict_anime[user] = anime_list\n",
    "    \n",
    "    generate_global_counts(dict_anime, global_candidates)\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: {('Toradora!',): 66719,\n",
       "  ('Naruto',): 66343,\n",
       "  ('Death Note',): 85642,\n",
       "  ('Code Geass: Hangyaku no Lelouch',): 71414,\n",
       "  ('Code Geass: Hangyaku no Lelouch R2',): 58654,\n",
       "  ('Sword Art Online',): 65842,\n",
       "  ('Shingeki no Kyojin',): 65821,\n",
       "  ('Angel Beats!',): 65303,\n",
       "  ('Bleach',): 63861,\n",
       "  ('Fullmetal Alchemist',): 61241,\n",
       "  ('Elfen Lied',): 68075,\n",
       "  ('Suzumiya Haruhi no Yuuutsu',): 60018,\n",
       "  ('Tengen Toppa Gurren Lagann',): 58929,\n",
       "  ('Clannad',): 63023,\n",
       "  ('Soul Eater',): 59331,\n",
       "  ('Fullmetal Alchemist: Brotherhood',): 67132,\n",
       "  ('Steins;Gate',): 60038,\n",
       "  ('Durarara!!',): 59267},\n",
       " 2: {('Code Geass: Hangyaku no Lelouch', 'Death Note'): 62237,\n",
       "  ('Death Note', 'Elfen Lied'): 59094,\n",
       "  ('Death Note', 'Fullmetal Alchemist: Brotherhood'): 58501},\n",
       " 3: {},\n",
       " 4: {},\n",
       " 5: {},\n",
       " 6: {}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "global_frequent_itemsets = {level: {itemset: count for itemset, count in itemsets.items() if count >= min_support_count} for level, itemsets in global_counts.items()}\n",
    "global_frequent_itemsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8746 candidate itemsets from 1st Level\n",
      "Found 18 frequent itemsets from 1th item candidate sets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 153/153 [00:01<00:00, 104.79it/s]\n",
      "0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('dataset/final_animedataset.csv')\n",
    "data = data[['username', 'title']]\n",
    "grouped_data = data.groupby('username')['title'].apply(list)\n",
    "grouped_data = grouped_data.to_dict()\n",
    "improved_apriori = Improved_Apriori(grouped_data, min_support=min_support, min_confidence=1, verbose=0)\n",
    "frequent_anime_set = improved_apriori.apriori()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: {('Angel Beats!',): 65303,\n",
       "  ('Bleach',): 63861,\n",
       "  ('Clannad',): 63023,\n",
       "  ('Code Geass: Hangyaku no Lelouch',): 71414,\n",
       "  ('Code Geass: Hangyaku no Lelouch R2',): 58654,\n",
       "  ('Death Note',): 85642,\n",
       "  ('Durarara!!',): 59267,\n",
       "  ('Elfen Lied',): 68075,\n",
       "  ('Fullmetal Alchemist',): 61241,\n",
       "  ('Fullmetal Alchemist: Brotherhood',): 67132,\n",
       "  ('Naruto',): 66343,\n",
       "  ('Shingeki no Kyojin',): 65821,\n",
       "  ('Soul Eater',): 59331,\n",
       "  ('Steins;Gate',): 60038,\n",
       "  ('Suzumiya Haruhi no Yuuutsu',): 60018,\n",
       "  ('Sword Art Online',): 65842,\n",
       "  ('Tengen Toppa Gurren Lagann',): 58929,\n",
       "  ('Toradora!',): 66719},\n",
       " 2: {('Code Geass: Hangyaku no Lelouch', 'Death Note'): 62237,\n",
       "  ('Death Note', 'Elfen Lied'): 59094,\n",
       "  ('Death Note', 'Fullmetal Alchemist: Brotherhood'): 58501}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frequent_anime_set"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
