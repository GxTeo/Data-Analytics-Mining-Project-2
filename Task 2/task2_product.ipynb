{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting efficient-aprioriNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Downloading efficient_apriori-2.0.3-py3-none-any.whl (14 kB)\n",
      "Installing collected packages: efficient-apriori\n",
      "Successfully installed efficient-apriori-2.0.3\n"
     ]
    }
   ],
   "source": [
    "pip install efficient-apriori"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "import sys\n",
    "sys.path.append('../Task 1/')\n",
    "from efficient_apriori import apriori\n",
    "from improved_apriori import Improved_Apriori\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "import itertools\n",
    "import ast\n",
    "from tqdm import tqdm\n",
    "import collections\n",
    "import math\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the dataset by chunks into username and the list of animes the user watched\n",
    "def process_anime_chunk(df, carry_over):\n",
    "    if carry_over is not None:\n",
    "        df = pd.concat([carry_over, df])\n",
    "    groups = df.groupby('user_id')['product_id'].apply(list)\n",
    "    last_user = df.iloc[-1]['user_id']\n",
    "    if last_user in groups:\n",
    "        carry_over = df[df['user_id'] == last_user]\n",
    "        groups = groups.drop(last_user)\n",
    "    else:\n",
    "        carry_over = None\n",
    "    return groups, carry_over"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "carry_over = None\n",
    "chunksize = 100000 # adjust this value depending on your available memory\n",
    "# Might have to figure out a way to shuffle the dataset \n",
    "if(not os.path.exists('dataset/processed_product_output.txt')):\n",
    "    with open('dataset/processed_product_output.txt', 'w') as f:\n",
    "        for chunk in pd.read_csv('2019-Nov.csv', chunksize=chunksize):\n",
    "            groups, carry_over = process_anime_chunk(chunk, carry_over)\n",
    "            for user, anime_list in groups.items():\n",
    "                f.write(f'{user} {anime_list}\\n')\n",
    "\n",
    "        # don't forget to process the last carry_over\n",
    "        if carry_over is not None:\n",
    "            groups, _ = process_anime_chunk(carry_over, None)\n",
    "            for user, anime_list in groups.items():\n",
    "                f.write(f'{user} {anime_list}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the text in chunks \n",
    "def shuffle_large_file(file_name, output_file_name, chunk_size):\n",
    "    with open(file_name, 'r') as f:\n",
    "        while True:\n",
    "            lines = list(itertools.islice(f, chunk_size))\n",
    "            if not lines:\n",
    "                break\n",
    "            random.shuffle(lines)\n",
    "            with open(output_file_name, 'a') as out:\n",
    "                out.write(''.join(lines))\n",
    "\n",
    "\n",
    "# Call the function with your parameters\n",
    "if(not os.path.exists('processed_product_output_shuffled.txt')):\n",
    "    shuffle_large_file('dataset/processed_product_output.txt', 'processed_product_output_shuffled.txt', 3000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file_in_partitions(file_path, partition_size):\n",
    "    with open(file_path, 'r') as file:\n",
    "        partition = []\n",
    "        for line in file:\n",
    "            partition.append(line)\n",
    "            if len(partition) >= partition_size:\n",
    "                yield partition\n",
    "                partition = []\n",
    "        if partition:  # yield any remaining lines\n",
    "            yield partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variable to get the counts of all itemsets\n",
    "global_counts = {}\n",
    "def generate_global_counts(partition, global_candidates):\n",
    "\n",
    "    # For 1th itemset, generate the transaction id list for the ith partition \n",
    "    transaction_id_dict = collections.defaultdict(list)\n",
    "    for transaction_id in partition:\n",
    "        for item in partition[transaction_id]:\n",
    "            item_tuple = (item,)\n",
    "            transaction_id_dict[item_tuple].append(transaction_id)\n",
    "\n",
    "    # Filter based on the global candidates formed\n",
    "    transaction_ids_dict = {item: transaction_ids for item, transaction_ids in transaction_id_dict.items() if item in global_candidates[1]}\n",
    "\n",
    "    # Get the global count of all 1th itemset\n",
    "    for item in transaction_id_dict:\n",
    "        if(len(item) not in global_counts):\n",
    "            global_counts[len(item)] = {}\n",
    "        if(item not in global_counts[len(item)]):\n",
    "            global_counts[len(item)][item] = len(transaction_id_dict[item])\n",
    "        else:\n",
    "            global_counts[len(item)][item] += len(transaction_id_dict[item])\n",
    "\n",
    "    # Extend to find global count of all nth itemset from the global candidates\n",
    "    for i in tqdm(range(1, len(global_candidates))):\n",
    "        for itemset in global_candidates[i+1]:\n",
    "            transaction_ids = set(transaction_id_dict[(itemset[0],)])\n",
    "            for i in range(1, len(itemset)):\n",
    "                # We are only interested in the transactions where all items in itemset is present\n",
    "                transaction_ids = transaction_ids.intersection(set(transaction_ids_dict.get((itemset[i],), {})))\n",
    "            if(len(itemset) not in global_counts):\n",
    "                global_counts[len(itemset)] = {}\n",
    "\n",
    "            if(itemset not in global_counts[len(itemset)]):\n",
    "                global_counts[len(itemset)][itemset] = len(transaction_ids)\n",
    "            else:\n",
    "                global_counts[len(itemset)][itemset] += len(transaction_ids)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_13496/417548823.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mfile_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'output_shuffled.txt'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0msize_of_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mpartition_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m10000\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpartition_size\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0msize_of_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_13496/417548823.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mfile_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'output_shuffled.txt'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0msize_of_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mpartition_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m10000\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpartition_size\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0msize_of_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\encodings\\cp1252.py\u001b[0m in \u001b[0;36mdecode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mIncrementalDecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIncrementalDecoder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m     \u001b[1;32mdef\u001b[0m \u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcharmap_decode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdecoding_table\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "file_path = 'processed_product_output_shuffled.txt'\n",
    "size_of_data = sum(1 for line in open(file_path))\n",
    "\n",
    "partition_size = 10000\n",
    "if(partition_size < size_of_data):\n",
    "    num_partitions = size_of_data// partition_size\n",
    "else:\n",
    "    print('Size of partition exceeds size of data')\n",
    "print(num_partitions)\n",
    "partition_candidates = []\n",
    "global_candidates = collections.defaultdict(list)\n",
    "# Step 1: Partitioning\n",
    "min_support_range=np.arange(0.001, 0.005, 0.001)\n",
    "\n",
    "for min_support in min_support_range:\n",
    "    output = {}\n",
    "    global_min_support = math.ceil((min_support*size_of_data)/num_partitions)\n",
    "    start = time.time()\n",
    "    for i, partition in enumerate(read_file_in_partitions(file_path, partition_size)):\n",
    "        print(f'Partition {i+1}:')\n",
    "        dict_anime = {}\n",
    "        for line in partition:\n",
    "            user, anime_list_str = line.strip().split(' ', 1)\n",
    "            anime_list = ast.literal_eval(anime_list_str)\n",
    "            dict_anime[user] = anime_list\n",
    "    \n",
    "        improved_apriori = Improved_Apriori(dict_anime, min_support=min_support, min_confidence=1, verbose=0)\n",
    "        # Step 2: Retreieve frequent itemset per partition\n",
    "        partition_frequent_itemset = improved_apriori.apriori()\n",
    "        # Efficient Apriori for sanity check\n",
    "        # partition_frequent_itemset, _ = apriori(list(dict_anime.values()), min_support = min_support, verbosity=2)\n",
    "\n",
    "        # Form the global candidate set from the large itemset in each partitions\n",
    "        # In this space, we ignore the count of itemset in each partition as they are not useful in our global support count\n",
    "        # All they do is just show the itemset was large enough in the current partition\n",
    "        # Merging Phase\n",
    "        for level, itemset in partition_frequent_itemset.items():\n",
    "            for key in itemset.keys():\n",
    "                if(key not in global_candidates[level]):\n",
    "                    global_candidates[level].append(key)\n",
    "\n",
    "    min_support_count = min_support * size_of_data                \n",
    "    # Global counting phase\n",
    "    global_counts = {}\n",
    "    for i, partition in enumerate(read_file_in_partitions(file_path, partition_size)):\n",
    "        print(f'Partition {i+1}:')\n",
    "        # Hold the partition data in main memory\n",
    "        dict_anime = {}\n",
    "        for line in partition:\n",
    "            user, anime_list_str = line.strip().split(' ', 1)\n",
    "            anime_list = ast.literal_eval(anime_list_str)\n",
    "            dict_anime[user] = anime_list\n",
    "        # Pure disk based implementation would probably require us to save the global candidates in disk \n",
    "        generate_global_counts(dict_anime, global_candidates)\n",
    "    global_frequent_itemsets = {level: {itemset: count for itemset, count in itemsets.items() if count >= min_support_count} for level, itemsets in global_counts.items()}\n",
    "    end = time.time()\n",
    "    output[min_support]={}\n",
    "    for level, itemsets in global_frequent_itemsets.items():\n",
    "        output[min_support][f'Level {level}'] = []\n",
    "        for items, count in itemsets.items():\n",
    "            output[min_support][f'Level {level}'].append([(list(items), count)])\n",
    "\n",
    "    output[min_support]['Time Taken'] = end-start\n",
    "\n",
    "    # Save to JSON file\n",
    "    with open(f'output_ecommerce_{min_support}.json', 'w') as f:\n",
    "        json.dump(output, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = pd.read_csv('dataset/final_animedataset.csv')\n",
    "# data = data[['username', 'title']]\n",
    "# grouped_data = data.groupby('username')['title'].apply(list)\n",
    "# grouped_data = grouped_data.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# frequent_anime_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# itemset, _ = apriori(list(grouped_data.values()), min_support = min_support, verbosity=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# itemset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
